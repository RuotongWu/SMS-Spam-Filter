{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader:\n",
    "    '''\n",
    "    data memebers:\n",
    "        self.tags\n",
    "        self.messages\n",
    "        self.word_voca\n",
    "        self.char_voca\n",
    "        self.embeddings\n",
    "        self.word_dic\n",
    "        self.embedded_message\n",
    "    '''\n",
    "\n",
    "    def __init__(self,path,embedding_path = os.getcwd() + \"/data/glove.840B.300d.txt\",lower = False):\n",
    "\n",
    "        self.tags = []\n",
    "        self.messages = []\n",
    "        with open(path) as f:\n",
    "            contents = f.readlines()\n",
    "            for line in contents:\n",
    "                #line = self.__clean_line(line)\n",
    "                tag, message = line.strip().split(\"\\t\")\n",
    "                self.tags.append(tag)\n",
    "                self.messages.append(self.__clean_line(message,lower))\n",
    "        self.__build_voc()\n",
    "        self.__build_vector_dict(embedding_path)\n",
    "        self.__convert_message()\n",
    "        self.__convert_tag()\n",
    "        print(\"init finished\")\n",
    "\n",
    "    def __build_voc(self):\n",
    "        '''\n",
    "        _build_voc will extract the vocabulary from the messages data\n",
    "        '''\n",
    "        counter_words = Counter()\n",
    "        for message in self.messages:\n",
    "            counter_words.update(message.strip().split())\n",
    "\n",
    "        self.word_voca = list(counter_words)\n",
    "\n",
    "        counter_chars = Counter()\n",
    "        for word in self.word_voca:\n",
    "            counter_chars.update(list(word))\n",
    "        self.char_voca =list(counter_chars)\n",
    "\n",
    "    def __clean_line(self, line,lower):\n",
    "        '''\n",
    "        remove punctuations\n",
    "        '''\n",
    "        line = line.encode('ascii',errors='ignore').decode()\n",
    "        line = re.sub(r\"([?.!,¿])\", r\" \\1 \", line)\n",
    "        line = re.sub(r'[\" \"]+', \" \", line)\n",
    "        line = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", line)\n",
    "        if lower:\n",
    "            line = line.lower()\n",
    "        return line\n",
    "\n",
    "    def __build_vector_dict(self, embedding_path):\n",
    "        self.word_dic = {}\n",
    "        for index, word in enumerate(self.word_voca):\n",
    "            self.word_dic[word] = index\n",
    "\n",
    "\n",
    "        print(\"Start to build word embedding vectors, this may take a while...\")\n",
    "        with open(embedding_path) as f:\n",
    "            vectors = f.readlines()\n",
    "            print(\"Embedding file loaded\")\n",
    "            dimension = len(vectors[0].strip().split())-1\n",
    "            embeddings = np.zeros((len(self.word_voca),dimension))\n",
    "            found = 0\n",
    "            for line in vectors:\n",
    "                line = line.strip().split()\n",
    "                if len(line)!= dimension +1:\n",
    "                    continue\n",
    "                word = line[0]\n",
    "                embedding = line[1:]\n",
    "                if word in self.word_dic:\n",
    "                    found += 1\n",
    "                    embeddings[self.word_dic[word]] = embedding\n",
    "            self.embeddings = embeddings\n",
    "\n",
    "    def data_split(self, ration = 0.9, random = False, embedding = True):\n",
    "        len_tags = len(self.tags)\n",
    "        len_messages = len(self.messages)\n",
    "        if embedding:\n",
    "            tags = self.embedded_tags\n",
    "            messages = self.embedded_messages\n",
    "        else:\n",
    "            #print(\"why else??!!??!!\")\n",
    "            tags = self.tags\n",
    "            messages = self.messages\n",
    "            \n",
    "        if len_tags != len_messages:\n",
    "            print(\"The number of tags doesn't equal to the number of messages, please check the file\")\n",
    "            return\n",
    "        if random:\n",
    "            tags,messages = self.__shuffle_data(tags,messages)\n",
    "        train_message = np.array(messages[:int(len_messages*0.9)])\n",
    "        train_tag = np.array(tags[:int(len_tags*0.9)])\n",
    "        test_message = np.array(messages[int(len_messages*0.9):])\n",
    "        test_tag = np.array(tags[int(len_tags*0.9):])\n",
    "\n",
    "        return train_message, train_tag, test_message, test_tag\n",
    "\n",
    "    #TODO shuffle the data when the dataset is different\n",
    "    def __shuffle_data(self,tags,messages):\n",
    "        data_type = type(tags)\n",
    "        combined_list = list(zip(tags, messages))\n",
    "        random.shuffle(combined_list)\n",
    "        tags, messages = zip(*combined_list)\n",
    "        tags = data_type(list(tags))\n",
    "        messages = data_type(list(messages))                  \n",
    "        return tags,messages\n",
    "    \n",
    "    def __convert_message(self):\n",
    "        converted_message = np.zeros((len(self.messages),128,len(self.embeddings[0])))\n",
    "        #print(converted_message.shape)\n",
    "        for mess_index,message in enumerate(self.messages):\n",
    "            #print(mess_index,message)\n",
    "            message = message.strip().split()\n",
    "            message_convert = np.zeros((128,len(self.embeddings[0])))\n",
    "            for index,word in enumerate(message):\n",
    "                if index >= 128:\n",
    "                    break\n",
    "                message_convert[index] = self.embeddings[self.word_dic[word]]\n",
    "            #print(converted_message.shape)\n",
    "            #print(message_convert.shape)\n",
    "            converted_message[mess_index] = message_convert\n",
    "            #np.append(converted_message,message_convert,axis = 0)\n",
    "            #converted_message.append(np.array(message_convert),axis = 0)   \n",
    "        #print(converted_message.shape)\n",
    "        self.embedded_messages = converted_message\n",
    "        \n",
    "    def __convert_tag(self):\n",
    "        self.embedded_tags = list(map(lambda x: int(x==\"spam\"),self.tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = data_loader(os.getcwd() + \"/data/SMSSpamCollection\",lower=True)\n",
    "#print(test.tags)\n",
    "#print(test.char_voca)\n",
    "#test.__build_voc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "spam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
